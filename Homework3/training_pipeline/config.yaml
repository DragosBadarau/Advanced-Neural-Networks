dataset: "CIFAR100"          # Choose between MNIST, CIFAR10, CIFAR100
model: "resnet18"           # Options: resnet18, preactresnet18 for CIFAR; mlp, lenet for MNIST
batch_size: 32
learning_rate: 0.075
epochs: 50
cache_data: True
device: "cuda"              # Options: 'cpu' or 'cuda'

# Optimizer settings
optimizer:
  type: "adam"               # Options: sgd, sgd_momentum, sgd_nesterov, sgd_weight_decay, adam, adamw, rmsprop
  momentum: 0.5              # Used for SGD optimizers
  weight_decay: 0.001       # Used for weight decay in SGD or AdamW
  nesterov: false           # Used for SGD with Nesterov

# Learning Rate Scheduler settings
#lr_scheduler:
#  type: "StepLR"              # Options: StepLR, ReduceLROnPlateau, or None
#  step_size: 5                # Step size for StepLR
#  gamma: 0.1                  # Decay factor for StepLR
#  patience: 3                 # Patience for ReduceLROnPlateau
#  factor: 0.1                 # Factor by which the learning rate is reduced in ReduceLROnPlateau
#  mode: "min"                 # Mode for ReduceLROnPlateau (either "min" or "max")


lr_scheduler:
  type: "ReduceLROnPlateau"
  patience: 3
  factor: 0.1
  mode: "max"

# Early stopping

early_stopping:
  enabled: true
  patience: 7           # Number of epochs with no improvement after which training will be stopped
  min_delta: 0.01       # Minimum change to qualify as an improvement
  mode: "max"           # "min" for validation loss, "max" for validation accuracy

# Data Augmentation settings
data_augmentation:
  scheme: "none"      # Options: "standard", "advanced", "none"

# Logging settings
logging:
  tensorboard: true
  wandb: true
  wandb_project: "hw3_pipeline"